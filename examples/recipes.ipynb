{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A gentle introduction to creation of neural networks reflexing structure of JSON documents \n",
    "\n",
    "This notebook serves as an introduction to Mill and JsonGrinder libraries. The former provides support for Multi-instance learning problems, their cascades, and their Cartesian product (see the paper... for theoretical explanation). The latter *JsonGrinder* simplifies processing of JSON documents. It allows to infer schema of JSON documents from which it suggests an extractor to convert JSON document to a structure in a *Mill*.*JsonGrinder* defines basic set of \"extractors\" converting values of keys to numeric representation (matrices) or to convert them to corresponding structures in *Mill*. Naturally, this set of extractors can be extended.\n",
    "\n",
    "Below, the intended workflow is demonstrated on a simple problem of guessing type of a cuisine from a list of ingrediences. Note that the goal is not to achieve state of the art, but to demonstrate the workflow.\n",
    "\n",
    "**Caution**\n",
    "To decrease the computational load in OceanCode, we decrease the number samples (200), size of the minibatch (10), and size of the validation data (100). Of course these numbers are useless in practice, and therefore the resulting accuracy is poor. Using all samples (666920), setting minibatch size to 100, and leaving 1000 samples for validation / testing gives you accuracy 0.74 on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666920, 100, 1000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nsamples, minisize, vsamples = 200, 10, 100 \n",
    "nsamples, minisize, vsamples = 666920, 100, 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the environment\n",
    "Let's start by replicating the development environment and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: new environment will be placed at /Users/tpevny/Work/Julia/Pkg/JsonGrinder/examples/JsonGrinder.jl\n",
      "└ @ Pkg.API /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v0.7/Pkg/src/API.jl:575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling JsonGrinder [d201646e-a9c0-11e8-1063-23b139159713]\n",
      "└ @ Base loading.jl:1187\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"JsonGrinder.jl\")\n",
    "Pkg.instantiate()\n",
    "using Revise, Flux, MLDataPattern, Mill, JsonGrinder, JSON, Statistics, Adapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data\n",
    "Data are stored in a format \"json per line\". This means that each sample is one JSON document stored in each line. These samples are loaded and parsed to an array. On the end, one sample is printed to show, how data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 39774-element Array{Dict,1} at index [1:666920]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 39774-element Array{Dict,1} at index [1:666920]",
      "",
      "Stacktrace:",
      " [1] throw_boundserror(::Array{Dict,1}, ::Tuple{UnitRange{Int64}}) at ./abstractarray.jl:492",
      " [2] checkbounds at ./abstractarray.jl:457 [inlined]",
      " [3] getindex(::Array{Dict,1}, ::UnitRange{Int64}) at ./array.jl:737",
      " [4] top-level scope at In[3]:4"
     ]
    }
   ],
   "source": [
    "samples = open(\"recipes.json\",\"r\") do fid \n",
    "\tArray{Dict}(JSON.parse(read(fid, String)))\n",
    "end;\n",
    "samples = samples[1:nsamples]\n",
    "JSON.print(samples[1],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unline XML or ProtoBuf, JSON documents do not have any schema. Threfore *JsonGrinder* attempts to infer the schema, which is then used to recommend the extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34m[Dict]\u001b[39m\n",
       "\u001b[34m  ├── \u001b[39m\u001b[39m    cuisine: [Scalar - String], 20 unique values, updated = 39774\n",
       "\u001b[34m  ├── \u001b[39m\u001b[39m         id: [Scalar - Int64], 1000 unique values, updated = 39774\n",
       "\u001b[34m  └── \u001b[39m\u001b[31mingredients: [List] (updated = 39774)\u001b[39m\n",
       "\u001b[34m      \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39m[Scalar - String], 1000 unique values, updated = 428275\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = JsonGrinder.schema(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the schema, we can create extractor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID is deleted from the schema (keys not in the schema are not reflected into extractor and hence not propagated into dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34m  ├── \u001b[39m\u001b[39m    cuisine: String\n",
       "\u001b[34m  └── \u001b[39m\u001b[39mingredients: Array of \u001b[39mString\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete!(schema.childs,\"id\");\n",
    "extractor = JsonGrinder.suggestextractor(Float32,schema,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cuisine is a class-label, the extractor needs to be split into two. `extract_data` will extract the sample and `extract_target` will extract the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data = JsonGrinder.ExtractBranch(nothing,deepcopy(extractor.other));\n",
    "extract_target = JsonGrinder.ExtractBranch(nothing,deepcopy(extractor.other));\n",
    "delete!(extract_target.other,\"ingredients\");\n",
    "delete!(extract_data.other,\"cuisine\");\n",
    "extract_target.other[\"cuisine\"] = JsonGrinder.ExtractCategorical(keys(schema.childs[\"cuisine\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `extract_data` is a functor extracting samples and `extract_target` extract targets. Let's first demonstrate extractor of datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagNode with 39774 bag(s)\u001b[39m\n",
       "\u001b[34m  └── \u001b[39m\u001b[39mArrayNode(1, 428275)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cat(map(extract_data, samples)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractor has returned structure containing 39774 samples (bags in MIL nomenclature). In total, 39774 samples contains 428275 instances.\n",
    "\n",
    "Let's investigate the first samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"romaine lettuce\" \"black olives\" \"grape tomatoes\" \"garlic\" \"pepper\" \"purple onion\" \"seasoning\" \"garbanzo beans\" \"feta cheese crumbles\"]"
     ]
    }
   ],
   "source": [
    "show(data[1].data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first sample contain nine instances. But at the moment, they are stored as string, which is difficult to be processed by a machine. This representation might be advantageous, as it saves memory and it can be converted to matrix format just before processing. \n",
    "\n",
    "To represent list of ingrediences as vectors, we define function `sentence2ngrams`, which split each ingredient to a set of words and represent each word by a histogram of trigrams. To decrease the number of trigrams, their index is the remainder after division `(modulo)`.\n",
    "\n",
    "The function is applied on data using `mapdata` function provided by the library.\n",
    "Be aware that this step might be time consuming..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagNode with 39774 bag(s)\u001b[39m\n",
       "\u001b[34m  └── \u001b[39m\u001b[31mBagNode with 428275 bag(s)\u001b[39m\n",
       "\u001b[34m      \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39mArrayNode(2057, 807760)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sentence2ngrams(ss::Array{T,N}) where {T<:AbstractString,N}\n",
    "\tfunction f(s)\n",
    "\t\tx = JsonGrinder.string2ngrams(split(s),3,2057)\n",
    "\t\tMill.BagNode(Mill.ArrayNode(x),[1:size(x,2)])\n",
    "\tend\n",
    "\tcat(map(f,ss)...)\n",
    "end\n",
    "sentence2ngrams(x) = x\n",
    "\n",
    "data = Mill.mapdata(sentence2ngrams,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at this moment, the sample consists of two MIL problems. Firstly, each dish is described by a set of ingrediences. Secondly, each ingredience is described by a set of words `e.g. [\"black\",\"olives\"]`. Finally, each word is represented as a vector of dimension 2057.\n",
    "\n",
    "Since histograms are sparse, to save memory and improve computational efficiency, we convert the data to SparseMatrix. Notive that the shape of data has not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagNode with 39774 bag(s)\u001b[39m\n",
       "\u001b[34m  └── \u001b[39m\u001b[31mBagNode with 428275 bag(s)\u001b[39m\n",
       "\u001b[34m      \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39mArrayNode(2057, 807760)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Mill.mapdata(i -> Mill.sparsify(Float32.(i),0.05),data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing the neural network the number of classes the classifier should recognize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = cat(map(extract_target, samples)...);\n",
    "odim = size(target.data,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model reflecting the structure of data\n",
    "\n",
    "Since manually creating a model reflecting the structure can be tedious, Mill support a semi-automated procedure. The function `reflectinmodel` takes as an input data sample and function, which for a given input dimension provides a feed-forward network. In the example below, the function creates a FeedForward network with a single layer with twenty neurons and relu nonlinearinty. \n",
    "\n",
    "Layers are wrapped to Chain (from Flux library), such that the last layer can be easily extended by the LinearLayer with appropriate number of output neurons, corresponding to number of target classes.\n",
    "\n",
    "The structure of the network (output [12]) corresponds to the  structure of input data (Output [10]). You can observe that each module dealing with multiple-instance data contains an aggregation layer with element-wise mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagModel\u001b[39m\n",
       "\u001b[34m  ├── \u001b[39m\u001b[31mBagModel\u001b[39m\n",
       "\u001b[34m  │   \u001b[39m\u001b[31m  ├── \u001b[39m\u001b[39mChain(Dense(2057, 80, NNlib.relu))\n",
       "\u001b[34m  │   \u001b[39m\u001b[31m  ├── \u001b[39m\u001b[39mAggregation((Mill._segmented_mean,))\n",
       "\u001b[34m  │   \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39mChain(Dense(80, 80, NNlib.relu))\n",
       "\u001b[34m  ├── \u001b[39m\u001b[39mAggregation((Mill._segmented_mean,))\n",
       "\u001b[34m  └── \u001b[39m\u001b[39mChain(Dense(80, 80, NNlib.relu), Dense(80, 20))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,k = Mill.reflectinmodel(data[1], k -> Chain(Dense(k,80,relu)));\n",
    "push!(m,Dense(k,odim));\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Mill library is compatible with MLDataPattern for manipulating with data (training / testing / minibatchsize preparation) and with Flux. Please, refer to thos two libraries for support.\n",
    "\n",
    "Below, data are first split into training and validation sets. Then Adam optimizer for training the model is initialized, and after defining intermediate output the data model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.076\n",
      "accuracy = 0.32\n",
      "accuracy = 0.517\n",
      "accuracy = 0.541\n",
      "accuracy = 0.57\n",
      "accuracy = 0.575\n",
      "accuracy = 0.609\n",
      "accuracy = 0.591\n",
      "accuracy = 0.614\n",
      "accuracy = 0.635\n",
      "accuracy = 0.639\n",
      "accuracy = 0.64\n",
      "accuracy = 0.66\n",
      "accuracy = 0.667\n",
      "accuracy = 0.685\n",
      "accuracy = 0.658\n",
      "accuracy = 0.674\n",
      "accuracy = 0.678\n",
      "accuracy = 0.672\n",
      "accuracy = 0.66\n",
      "accuracy = 0.684\n",
      "accuracy = 0.678\n",
      "accuracy = 0.675\n",
      "accuracy = 0.691\n",
      "accuracy = 0.702\n",
      "accuracy = 0.695\n",
      "accuracy = 0.697\n",
      "accuracy = 0.699\n",
      "accuracy = 0.712\n",
      "accuracy = 0.709\n",
      "accuracy = 0.71\n",
      "accuracy = 0.708\n",
      "accuracy = 0.722\n",
      "accuracy = 0.699\n",
      "accuracy = 0.71\n",
      "accuracy = 0.71\n",
      "accuracy = 0.713\n",
      "accuracy = 0.707\n",
      "accuracy = 0.708\n",
      "accuracy = 0.718\n",
      "accuracy = 0.719\n",
      "accuracy = 0.721\n",
      "accuracy = 0.705\n",
      "accuracy = 0.718\n",
      "accuracy = 0.717\n",
      "accuracy = 0.701\n",
      "accuracy = 0.713\n",
      "accuracy = 0.707\n",
      "accuracy = 0.708\n",
      "accuracy = 0.727\n",
      "accuracy = 0.725\n",
      "accuracy = 0.715\n",
      "accuracy = 0.721\n",
      "accuracy = 0.721\n",
      "accuracy = 0.717\n",
      "accuracy = 0.719\n",
      "accuracy = 0.715\n",
      "accuracy = 0.726\n",
      "accuracy = 0.718\n",
      "accuracy = 0.718\n",
      "accuracy = 0.729\n",
      "accuracy = 0.718\n",
      "accuracy = 0.739\n",
      "accuracy = 0.731\n",
      "accuracy = 0.741\n"
     ]
    }
   ],
   "source": [
    "valdata = data[1:vsamples],target[1:vsamples]\n",
    "data, target = data[vsamples + 1:nobs(data)], target[vsamples + 1:nobs(target)]\n",
    "opt = Flux.Optimise.ADAM(params(m))\n",
    "loss = (x,y) -> Flux.logitcrossentropy(m(getobs(x)).data,getobs(y).data) \n",
    "cb = () -> println(\"accuracy = \",mean(Flux.onecold(Flux.data(m(valdata[1]).data)) .== Flux.onecold(valdata[2].data)))\n",
    "Flux.Optimise.train!(loss, RandomBatches((data,target),10,10000), opt, cb = Flux.throttle(cb, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting accuracy on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.684\n"
     ]
    }
   ],
   "source": [
    "println(\"accuracy = \",mean(Flux.onecold(Flux.data(m(valdata[1]).data)) .== Flux.onecold(valdata[2].data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
