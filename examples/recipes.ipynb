{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A gentle introduction to creation of neural networks reflexing structure of JSON documents \n",
    "\n",
    "This notebook serves as an introduction to Mill (https://github.com/pevnak/Mill.jl) and JsonGrinder (https://github.com/pevnak/JsonGrinder) libraries. The former provides support for Multi-instance learning problems, their cascades, and their Cartesian product (see the paper... for theoretical explanation). The latter *JsonGrinder* simplifies processing of JSON documents. It allows to infer schema of JSON documents from which it suggests an extractor to convert JSON document to a structure in a *Mill*.*JsonGrinder* defines basic set of \"extractors\" converting values of keys to numeric representation (matrices) or to convert them to corresponding structures in *Mill*. Naturally, this set of extractors can be extended.\n",
    "\n",
    "Below, the intended workflow is demonstrated on a simple problem of guessing type of a cuisine from a list of ingrediences. Note that the goal is not to achieve state of the art, but to demonstrate the workflow.\n",
    "\n",
    "A basic knowledge of Julia and Flux library certainly improves the understanding...\n",
    "\n",
    "Let's start by importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise, Flux, MLDataPattern, Mill, JsonGrinder, FluxExtensions, JSON, Statistics, Adapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are stored in a format \"json per line\". This means that each sample is one JSON document stored in each line. These samples are loaded and parsed to an array. On the end, one sample is printed to show, how data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 10259,\n",
      "  \"ingredients\": [\n",
      "    \"romaine lettuce\",\n",
      "    \"black olives\",\n",
      "    \"grape tomatoes\",\n",
      "    \"garlic\",\n",
      "    \"pepper\",\n",
      "    \"purple onion\",\n",
      "    \"seasoning\",\n",
      "    \"garbanzo beans\",\n",
      "    \"feta cheese crumbles\"\n",
      "  ],\n",
      "  \"cuisine\": \"greek\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "samples = open(\"recipes.json\",\"r\") do fid \n",
    "\tArray{Dict}(JSON.parse(read(fid, String)))\n",
    "end;\n",
    "JSON.print(samples[1],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unline XML or ProtoBuf, JSON documents do not have any schema. Threfore *JsonGrinder* attempts to infer the schema, which is then used to recommend the extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34m[Dict]\u001b[39m\n",
       "\u001b[34m  ├── \u001b[39m\u001b[39m    cuisine: [Scalar - String], 20 unique values, updated = 39774\n",
       "\u001b[34m  ├── \u001b[39m\u001b[39m         id: [Scalar - Int64], 1000 unique values, updated = 39774\n",
       "\u001b[34m  └── \u001b[39m\u001b[31mingredients: [List] (updated = 39774)\u001b[39m\n",
       "\u001b[34m      \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39m[Scalar - String], 1000 unique values, updated = 428275\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = JsonGrinder.schema(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the schema, we can create extractor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID is deleted from the schema (keys not in the schema are not reflected into extractor and hence not propagated into dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34m  ├── \u001b[39m\u001b[39m    cuisine: String\n",
       "\u001b[34m  └── \u001b[39m\u001b[39mingredients: Array of \u001b[39mString\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete!(schema.childs,\"id\");\n",
    "extractor = JsonGrinder.suggestextractor(Float32,schema,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cuisine is a class-label, the extractor needs to be split into two. `extract_data` will extract the sample and `extract_target` will extract the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data = JsonGrinder.ExtractBranch(nothing,deepcopy(extractor.other));\n",
    "extract_target = JsonGrinder.ExtractBranch(nothing,deepcopy(extractor.other));\n",
    "delete!(extract_target.other,\"ingredients\");\n",
    "delete!(extract_data.other,\"cuisine\");\n",
    "extract_target.other[\"cuisine\"] = JsonGrinder.ExtractCategorical(keys(schema.childs[\"cuisine\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `extract_data` is a functor extracting samples and `extract_target` extract targets. Let's first demonstrate extractor of datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagNode with 39774 bag(s)\u001b[39m\n",
       "\u001b[34m  └── \u001b[39m\u001b[39mArrayNode(1, 428275)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cat(map(extract_data, samples)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractor has returned structure containing 39774 samples (bags in MIL nomenclature). In total, 39774 samples contains 428275 instances.\n",
    "\n",
    "Let's investigate the first samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"romaine lettuce\" \"black olives\" \"grape tomatoes\" \"garlic\" \"pepper\" \"purple onion\" \"seasoning\" \"garbanzo beans\" \"feta cheese crumbles\"]"
     ]
    }
   ],
   "source": [
    "show(data[1].data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first sample contain nine instances. But at the moment, they are stored as string, which is difficult to be processed by a machine. This representation might be advantageous, as it saves memory and it can be converted to matrix format just before processing. \n",
    "\n",
    "To represent list of ingrediences as vectors, we define function `sentence2ngrams`, which split each ingredient to a set of words and represent each word by a histogram of trigrams. To decrease the number of trigrams, their index is the remainder after division `(modulo)`.\n",
    "\n",
    "The function is applied on data using `mapdata` function provided by the library.\n",
    "Be aware that this step might be time consuming..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagNode with 39774 bag(s)\u001b[39m\n",
       "\u001b[34m  └── \u001b[39m\u001b[31mBagNode with 428275 bag(s)\u001b[39m\n",
       "\u001b[34m      \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39mArrayNode(2057, 807760)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sentence2ngrams(ss::Array{T,N}) where {T<:AbstractString,N}\n",
    "\tfunction f(s)\n",
    "\t\tx = JsonGrinder.string2ngrams(split(s),3,2057)\n",
    "\t\tMill.BagNode(Mill.ArrayNode(x),[1:size(x,2)])\n",
    "\tend\n",
    "\tcat(map(f,ss)...)\n",
    "end\n",
    "sentence2ngrams(x) = x\n",
    "\n",
    "data = Mill.mapdata(sentence2ngrams,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at this moment, the sample consists of two MIL problems. Firstly, each dish is described by a set of ingrediences. Secondly, each ingredience is described by a set of words `e.g. [\"black\",\"olives\"]`. Finally, each word is represented as a vector of dimension 2057.\n",
    "\n",
    "Since histograms are sparse, to save memory and improve computational efficiency, we convert the data to SparseMatrix. Notive that the shape of data has not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagNode with 39774 bag(s)\u001b[39m\n",
       "\u001b[34m  └── \u001b[39m\u001b[31mBagNode with 428275 bag(s)\u001b[39m\n",
       "\u001b[34m      \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39mArrayNode(2057, 807760)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Mill.mapdata(i -> Mill.sparsify(Float32.(i),0.05),data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since manually creating a model reflecting the structure can be tedious, Mill support a semi-automated procedure. The function `reflectinmodel` takes as an input data sample and function, which for a given input dimension provides a feed-forward network. In the example below, the function creates a FeedForward network with a single layer with twenty neurons and relu nonlinearinty. Note, that the layer is wrapped to Chain (from Flux library), such that the last layer can be easily extended by the LinearLayer with 18 output neurons, corresponding to 18 classes target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mBagModel\u001b[39m\n",
       "\u001b[34m  ├── \u001b[39m\u001b[31mBagModel\u001b[39m\n",
       "\u001b[34m  │   \u001b[39m\u001b[31m  ├── \u001b[39m\u001b[39mChain(Dense(2057, 20, NNlib.relu))\n",
       "\u001b[34m  │   \u001b[39m\u001b[31m  ├── \u001b[39m\u001b[39mAggregation((Mill.segmented_mean,))\n",
       "\u001b[34m  │   \u001b[39m\u001b[31m  └── \u001b[39m\u001b[39mChain(Dense(20, 20, NNlib.relu))\n",
       "\u001b[34m  ├── \u001b[39m\u001b[39mAggregation((Mill.segmented_mean,))\n",
       "\u001b[34m  └── \u001b[39m\u001b[39mChain(Dense(20, 20, NNlib.relu), Dense(20, 20))\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,k = Mill.reflectinmodel(data[1], k -> Chain(Dense(k,20,relu)));\n",
    "push!(m,Dense(k,20));\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the testing data, which is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[39mArrayNode(20, 39774)\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = cat(map(extract_target, samples)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are to go and train the model. We reserve first 1000 samples as validation, the rest will be used for training.\n",
    "\n",
    "Mill library is compatible with Flux infrastructure for training and with MLDataPattern library for managing training samples. Please, refer to thos two libraries for support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.092\n",
      "accuracy = 0.636\n",
      "accuracy = 0.657\n",
      "accuracy = 0.676\n",
      "accuracy = 0.688\n",
      "accuracy = 0.701\n",
      "accuracy = 0.712\n",
      "accuracy = 0.73\n",
      "accuracy = 0.722\n",
      "accuracy = 0.73\n",
      "accuracy = 0.725\n",
      "accuracy = 0.736\n",
      "accuracy = 0.738\n",
      "accuracy = 0.745\n",
      "accuracy = 0.733\n"
     ]
    }
   ],
   "source": [
    "opt = Flux.Optimise.ADAM(params(m))\n",
    "loss = (x,y) -> Flux.logitcrossentropy(m(getobs(x)).data,getobs(y).data) \n",
    "valdata = data[1:1000],target[1:1000]\n",
    "data, target = data[1001:nobs(data)], target[1001:nobs(target)]\n",
    "cb = () -> println(\"accuracy = \",mean(Flux.onecold(Flux.data(m(valdata[1]).data)) .== Flux.onecold(valdata[2].data)))\n",
    "Flux.Optimise.train!(loss, RandomBatches((data,target),100,10000), opt, cb = Flux.throttle(cb, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can report accuracy on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(Flux.onecold(Flux.data(m(valdata[1]).data)) .== Flux.onecold(valdata[2].data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
